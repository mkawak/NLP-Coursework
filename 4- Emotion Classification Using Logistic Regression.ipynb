{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Emotion Classification Using Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "nltk.download('punkt')\n"
   ],
   "metadata": {
    "id": "28MWPnsNfEbI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "46afe635-79a1-4a68-fd7f-8d23377de5e9",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:09:36.898458Z",
     "start_time": "2023-12-22T13:09:36.883048Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Read the Excel sheet into a pandas DataFrame\n",
    "df = pd.read_excel('Files/data.xlsx')\n",
    "\n",
    "# Select the first 30 rows of the DataFrame\n",
    "df = df.head(30)\n",
    "\n",
    "# Define the emotions to count\n",
    "emotions = ['Sadness', 'Joy', 'Fear', 'Anger', 'Surprise', 'Disgust']\n",
    "\n",
    "# Initialize a dictionary to store the sentence counts for each emotion\n",
    "emotion_sentences_counts = {emotion: 0 for emotion in emotions}\n",
    "\n",
    "# Loop through each even-numbered column in the DataFrame\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "    # Remove \"Sentences\" from the column name\n",
    "    emotion_name = col_name.replace(\" Sentences\", \"\")\n",
    "    # Check if the column contains any emotions to count\n",
    "    column_emotions = []\n",
    "    for emotion in emotions:\n",
    "        if emotion in emotion_name:\n",
    "            column_emotions.append(emotion)\n",
    "    # If the column contains at least one emotion to count, add the number of non-empty sentences to the count\n",
    "    if len(column_emotions) > 0:\n",
    "        non_empty_sentences = df[col_name].dropna().count()\n",
    "        for emotion in column_emotions:\n",
    "            emotion_sentences_counts[emotion] += non_empty_sentences\n",
    "        # Print the number of non-empty sentences for each emotion and column\n",
    "        print(f\"Column '{col_name}' has {non_empty_sentences} non-empty sentences.\")\n",
    "        for emotion in column_emotions:\n",
    "            print(f\" {non_empty_sentences} sentences added to '{emotion}' count\")\n",
    "\n",
    "# Get total count of sentences in \"Sadness\" + \"Joy\"\n",
    "total_count = emotion_sentences_counts['Sadness'] + emotion_sentences_counts['Joy']\n",
    "\n",
    "emotions_lexicon_bag_of_words = {emotion: [] for emotion in emotions}\n",
    "emotions_sentences_bag_of_words = {emotion: [] for emotion in emotions}\n",
    "\n",
    "# Loop through each emotion\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "\n",
    "    # Extract the emotion name from the column name\n",
    "    emotion = col_name.replace(' Sentences', '')\n",
    "    # Get the index of the \"Sentences\" column for this emotion\n",
    "    sentences_col_idx = df.columns.get_loc(col_name)\n",
    "    # Get emotion lexicon from the \"Lexicon\" column\n",
    "    lexicon = df.iloc[0:, sentences_col_idx-1].tolist()\n",
    "    # Get the sentences from the \"Sentences\" column\n",
    "    sentences = df.iloc[0:, sentences_col_idx].tolist()\n",
    "    # Create an empty list to store the lexicon\n",
    "    lex_tokens = []\n",
    "    # Create an empty list to store the sentences tokens\n",
    "    sentences_tokens = []\n",
    "    # Loop through each lexicon col\n",
    "    for lexeme in lexicon:\n",
    "        # Check if the lexicon is a valid string\n",
    "        if isinstance(lexeme, str):\n",
    "            # Tokenize the words in each row \"lexeme\"\n",
    "            tokens = nltk.word_tokenize(lexeme)\n",
    "            # Remove commas\n",
    "            tokens_without_commas = [token for token in tokens if token != ',']\n",
    "            # Append to array\n",
    "            lex_tokens.extend(tokens_without_commas)\n",
    "\n",
    "    # Set to lowercase and remove duplicates lexicon\n",
    "    lex_tokens = list(set(token.lower() for token in lex_tokens))\n",
    "    # Loop through each sentence col\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence is a valid string\n",
    "        if isinstance(sentence, str):\n",
    "          # Tokenize the words in each row \"sentence\"\n",
    "          tokens = nltk.word_tokenize(sentence)\n",
    "          # Append to array\n",
    "          sentences_tokens.append(tokens)\n",
    "    # If the emotion contains more than one emotion, split it and add the words to the respective index arrays\n",
    "    if '+' in emotion:\n",
    "        sub_emotions = emotion.split(' + ')\n",
    "        for sub_emotion in sub_emotions:\n",
    "            emotions_lexicon_bag_of_words[sub_emotion].extend(lex_tokens)\n",
    "            emotions_sentences_bag_of_words[sub_emotion].extend(sentences_tokens)\n",
    "    else:\n",
    "        emotions_lexicon_bag_of_words[emotion].extend(lex_tokens)\n",
    "        emotions_sentences_bag_of_words[emotion].extend(sentences_tokens)\n",
    "\n",
    "# Remove duplicates from \"Sadness\" and \"Joy\" lexicon\n",
    "emotions_lexicon_bag_of_words['Sadness'] = list(set(emotions_lexicon_bag_of_words['Sadness']))\n",
    "emotions_lexicon_bag_of_words['Joy'] = list(set(emotions_lexicon_bag_of_words['Joy']))\n",
    "\n",
    "# Joy lexicon \n",
    "x1 = len(emotions_lexicon_bag_of_words['Joy'])\n",
    "# Sadness lexicon \n",
    "x2 = len(emotions_lexicon_bag_of_words['Sadness'])\n",
    "# Sadness sentences tokens + Sadness sentences tokens\n",
    "x3 = math.log(len(emotions_sentences_bag_of_words['Joy']) + len(emotions_sentences_bag_of_words['Sadness']))\n",
    "# y label\n",
    "y = 1\n",
    "# Weights vector\n",
    "w = [0, 0, 0]\n",
    "# Bias\n",
    "b = 0"
   ],
   "metadata": {
    "id": "itB9GvuMiO3u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "080d18ff-e56f-4f15-b2cc-e3b10e416378",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:09:36.958839Z",
     "start_time": "2023-12-22T13:09:36.886628Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Sadness Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Sadness' count\n",
      "Column 'Joy Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Joy' count\n",
      "Column 'Fear Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Fear' count\n",
      "Column 'Anger Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Anger' count\n",
      "Column 'Surprise Sentences' has 29 non-empty sentences.\n",
      " 29 sentences added to 'Surprise' count\n",
      "Column 'Disgust Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Sadness' count\n",
      " 30 sentences added to 'Joy' count\n",
      "Column 'Fear + Anger Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Fear' count\n",
      " 30 sentences added to 'Anger' count\n",
      "Column 'Surprise + Disgust Sentences' has 28 non-empty sentences.\n",
      " 28 sentences added to 'Surprise' count\n",
      " 28 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy + Fear Sentences' has 29 non-empty sentences.\n",
      " 29 sentences added to 'Sadness' count\n",
      " 29 sentences added to 'Joy' count\n",
      " 29 sentences added to 'Fear' count\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_regression_prob(x1, x2, x3, w, b):\n",
    "    # Combine the features into a single input vector\n",
    "    x = np.array([x1, x2, x3])\n",
    "\n",
    "    # Compute the logit (linear combination of features and weights)\n",
    "    logit = np.dot(x, w) + b\n",
    "\n",
    "    # Compute the probability using the sigmoid function\n",
    "    probability = sigmoid(logit)\n",
    "\n",
    "    return probability\n",
    "\n",
    "# Call the logistic regression classifier function\n",
    "joy_probability = logistic_regression_prob(x1, x2, x3, w, b)\n",
    "\n",
    "# Print the computed probability\n",
    "print(\"Joy probability = \" + str(joy_probability))\n",
    "print(\"Sadness probability = \" + str(1- joy_probability))\n",
    "\n",
    "\n",
    "# Compute Cross Entropu Loss\n",
    "def cross_entropy_loss(x1, x2, x3, w, b, y):\n",
    "\n",
    "    return -1*(y*math.log(logistic_regression_prob(x1, x2, x3, w, b)))+((1-y)*(1-logistic_regression_prob(x1, x2, x3, w, b)))\n",
    "\n",
    "loss_1 = cross_entropy_loss(x1, x2, x3, w, b, y)\n",
    "loss_2 = cross_entropy_loss(x1, x2, x3, w, b, 0)\n",
    "print(\"Loss when p(y = 1) = \" + str(loss_1))\n",
    "print(\"Loss when p(y = 0) = \" + str(loss_2))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i97MLoYLBr7L",
    "outputId": "093ca2d7-caf0-4a14-d6be-45715689e859",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:09:36.962502Z",
     "start_time": "2023-12-22T13:09:36.959357Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy probability = 0.5\n",
      "Sadness probability = 0.5\n",
      "Loss when p(y = 1) = 0.6931471805599453\n",
      "Loss when p(y = 0) = 0.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the Excel sheet into a pandas DataFrame\n",
    "df = pd.read_excel('Files/data.xlsx')\n",
    "\n",
    "# Select the next validation rows from index 30 to 40\n",
    "df = df.head(40).tail(10)\n",
    "\n",
    "# Define the emotions to count\n",
    "emotions = ['Sadness', 'Joy', 'Fear', 'Anger', 'Surprise', 'Disgust']\n",
    "\n",
    "# Initialize a dictionary to store the sentence counts for each emotion\n",
    "emotion_sentences_counts = {emotion: 0 for emotion in emotions}\n",
    "\n",
    "# Loop through each even-numbered column in the DataFrame\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "    # Remove \"Sentences\" from the column name\n",
    "    emotion_name = col_name.replace(\" Sentences\", \"\")\n",
    "    # Check if the column contains any emotions to count\n",
    "    column_emotions = []\n",
    "    for emotion in emotions:\n",
    "        if emotion in emotion_name:\n",
    "            column_emotions.append(emotion)\n",
    "    # If the column contains at least one emotion to count, add the number of non-empty sentences to the count\n",
    "    if len(column_emotions) > 0:\n",
    "        non_empty_sentences = df[col_name].dropna().count()\n",
    "        for emotion in column_emotions:\n",
    "            emotion_sentences_counts[emotion] += non_empty_sentences\n",
    "        # Print the number of non-empty sentences for each emotion and column\n",
    "        print(f\"Column '{col_name}' has {non_empty_sentences} non-empty sentences.\")\n",
    "        for emotion in column_emotions:\n",
    "            print(f\" {non_empty_sentences} sentences added to '{emotion}' count\")\n",
    "\n",
    "# Get total count of sentences in \"Sadness\" + \"Joy\"\n",
    "total_count = emotion_sentences_counts['Sadness'] + emotion_sentences_counts['Joy']\n",
    "\n",
    "emotions_lexicon_bag_of_words = {emotion: [] for emotion in emotions}\n",
    "emotions_sentences_bag_of_words = {emotion: [] for emotion in emotions}\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "# Loop through each emotion\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "\n",
    "    # Extract the emotion name from the column name\n",
    "    emotion = col_name.replace(' Sentences', '')\n",
    "    # Get the index of the \"Sentences\" column for this emotion\n",
    "    sentences_col_idx = df.columns.get_loc(col_name)\n",
    "    # Get emotion lexicon from the \"Lexicon\" column\n",
    "    lexicon = df.iloc[0:, sentences_col_idx-1].tolist()\n",
    "    # Get the sentences from the \"Sentences\" column\n",
    "    sentences = df.iloc[0:, sentences_col_idx].tolist()\n",
    "    # Create an empty list to store the lexicon\n",
    "    lex_tokens = []\n",
    "    # Create an empty list to store the sentences tokens\n",
    "    sentences_tokens = []\n",
    "    # Loop through each lexicon col\n",
    "    for lexeme in lexicon:\n",
    "        # Check if the lexicon is a valid string\n",
    "        if isinstance(lexeme, str):\n",
    "            # Tokenize the words in each row \"lexeme\"\n",
    "            tokens = nltk.word_tokenize(lexeme)\n",
    "            # Remove commas\n",
    "            tokens_without_commas = [token for token in tokens if token != ',']\n",
    "            # Append to array\n",
    "            lex_tokens.extend(tokens_without_commas)\n",
    "\n",
    "    # Set to lowercase and remove duplicates lexicon\n",
    "    lex_tokens = list(set(token.lower() for token in lex_tokens))\n",
    "    # Loop through each sentence col\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence is a valid string\n",
    "        if isinstance(sentence, str):\n",
    "          # Tokenize the words in each row \"sentence\"\n",
    "          tokens = nltk.word_tokenize(sentence)\n",
    "          # Append to array\n",
    "          sentences_tokens.append(tokens)\n",
    "    # If the emotion contains more than one emotion, split it and add the words to the respective index arrays\n",
    "    if '+' in emotion:\n",
    "        sub_emotions = emotion.split(' + ')\n",
    "        for sub_emotion in sub_emotions:\n",
    "            emotions_lexicon_bag_of_words[sub_emotion].extend(lex_tokens)\n",
    "            emotions_sentences_bag_of_words[sub_emotion].extend(sentences_tokens)\n",
    "    else:\n",
    "        emotions_lexicon_bag_of_words[emotion].extend(lex_tokens)\n",
    "        emotions_sentences_bag_of_words[emotion].extend(sentences_tokens)\n",
    "\n",
    "\n",
    "# Remove duplicates from \"Sadness\" and \"Joy\" lexicon\n",
    "emotions_lexicon_bag_of_words['Sadness'] = list(set(emotions_lexicon_bag_of_words['Sadness']))\n",
    "emotions_lexicon_bag_of_words['Joy'] = list(set(emotions_lexicon_bag_of_words['Joy']))\n",
    "\n",
    "# Joy lexicon \n",
    "x1 = len(emotions_lexicon_bag_of_words['Joy'])\n",
    "# Sadness lexicon \n",
    "x2 = len(emotions_lexicon_bag_of_words['Sadness'])\n",
    "# Sadness sentences tokens + Sadness sentences tokens\n",
    "x3 = math.log(len(emotions_sentences_bag_of_words['Joy']) + len(emotions_sentences_bag_of_words['Sadness']))\n",
    "# y label\n",
    "y = 1\n",
    "# Weights vector\n",
    "w = [0, 0, 0]\n",
    "# Bias\n",
    "b = 0\n",
    "\n",
    "# Compute Cross Entropy Loss\n",
    "def cross_entropy_loss(x1, x2, x3, w, b, y):\n",
    "\n",
    "    return -1*(y*math.log(logistic_regression_prob(x1, x2, x3, w, b)))+((1-y)*(1-logistic_regression_prob(x1, x2, x3, w, b)))\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(x1, x2, x3, y, learning_rate, iterations_num):\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(3)\n",
    "    b = 0\n",
    "\n",
    "    for i in range(iterations_num):\n",
    "        # Compute the predicted probability\n",
    "        prob = logistic_regression_prob(x1, x2, x3, w, b)\n",
    "\n",
    "        # Compute the gradients\n",
    "        gradient_w = (prob - y) * np.array([x1, x2, x3])\n",
    "        gradient_b = prob - y\n",
    "\n",
    "        # Update the weights and bias\n",
    "        w -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# Learning rates\n",
    "learning_rates= [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "# Number of iterations\n",
    "iterations_num = 10000\n",
    "\n",
    "best_probability = 0\n",
    "lowest_loss = float('inf')\n",
    "best_learning_rate = None\n",
    "best_w = None\n",
    "best_b = None\n",
    "learning_rate_losses = []\n",
    "\n",
    "# Iterate through learning rates\n",
    "for learning_rate in learning_rates:\n",
    "    # Perform stochastic gradient descent\n",
    "    w, b = stochastic_gradient_descent(x1, x2, x3, y, learning_rate, iterations_num)\n",
    "    # Compute the probability using the obtained weights and bias\n",
    "    probability = logistic_regression_prob(x1, x2, x3, w, b)\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = cross_entropy_loss(x1, x2, x3, w, b, y)\n",
    "    # Update the best results if the current probability is higher\n",
    "    if probability > best_probability:\n",
    "        best_probability = probability\n",
    "        best_learning_rate = learning_rate\n",
    "        best_w = w\n",
    "        best_b = b\n",
    "    # Update the lowest loss and corresponding learning rate if the current loss is lower\n",
    "    if loss < lowest_loss:\n",
    "        lowest_loss = loss\n",
    "        best_learning_rate = learning_rate\n",
    "        best_w = w\n",
    "        best_b = b\n",
    "    # Append the learning rate and validation loss to the list\n",
    "    learning_rate_losses.append((learning_rate, probability, loss))\n",
    "\n",
    "print(\"1. Best Learning Rate:\" + str(best_learning_rate))\n",
    "print(\"2. Lowest Validation Loss:\"+ str(lowest_loss) + \" for learning rate:\" + str(best_learning_rate))\n",
    "print(\"3. Best Probability:\" + str(best_probability) + \" for learning rate:\" + str( best_learning_rate))\n",
    "print(\"4. Best Weight Vector:\", best_w)\n",
    "print(\"5. Best Bias:\", best_b)\n",
    "print(\"6. List of all Learning Rates and Validation Losses:\")\n",
    "for rate, probability, loss in learning_rate_losses:\n",
    "    print(\"   Learning Rate:\" + str(\"{:.5f}\".format(rate)) + \" Probability:\" + str(\"{:.5f}\".format(probability)) + \" Validation Loss:\" + str(loss))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaL08MB_ZIyX",
    "outputId": "330b60b5-a1db-4687-feec-bfbac0ef5176",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:09:37.424025Z",
     "start_time": "2023-12-22T13:09:36.961408Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Sadness Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Sadness' count\n",
      "Column 'Joy Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Joy' count\n",
      "Column 'Fear Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Fear' count\n",
      "Column 'Anger Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Anger' count\n",
      "Column 'Surprise Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Surprise' count\n",
      "Column 'Disgust Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Sadness' count\n",
      " 10 sentences added to 'Joy' count\n",
      "Column 'Fear + Anger Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Fear' count\n",
      " 10 sentences added to 'Anger' count\n",
      "Column 'Surprise + Disgust Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Surprise' count\n",
      " 10 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy + Fear Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Sadness' count\n",
      " 10 sentences added to 'Joy' count\n",
      " 10 sentences added to 'Fear' count\n",
      "------------------------------------------\n",
      "1. Best Learning Rate:0.005\n",
      "2. Lowest Validation Loss:0.0 for learning rate:0.005\n",
      "3. Best Probability:1.0 for learning rate:0.005\n",
      "4. Best Weight Vector: [0.2325     0.235      0.01023586]\n",
      "5. Best Bias: 0.0025\n",
      "6. List of all Learning Rates and Validation Losses:\n",
      "   Learning Rate:0.00001 Probability:0.99943 Validation Loss:0.0005730966505509135\n",
      "   Learning Rate:0.00005 Probability:0.99989 Validation Loss:0.00011431814872493498\n",
      "   Learning Rate:0.00010 Probability:0.99994 Validation Loss:5.7134255689389406e-05\n",
      "   Learning Rate:0.00050 Probability:0.99999 Validation Loss:1.141437622383793e-05\n",
      "   Learning Rate:0.00100 Probability:0.99999 Validation Loss:5.514014356088016e-06\n",
      "   Learning Rate:0.00500 Probability:1.00000 Validation Loss:0.0\n",
      "   Learning Rate:0.01000 Probability:1.00000 Validation Loss:0.0\n",
      "   Learning Rate:0.05000 Probability:1.00000 Validation Loss:0.0\n",
      "   Learning Rate:0.10000 Probability:1.00000 Validation Loss:0.0\n",
      "   Learning Rate:0.50000 Probability:1.00000 Validation Loss:0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the Excel sheet into a pandas DataFrame\n",
    "df = pd.read_excel('Files/data.xlsx')\n",
    "\n",
    "# Select the next test rows from index 40 to 50\n",
    "df = df.tail(10)\n",
    "\n",
    "# Define the emotions to count\n",
    "emotions = ['Sadness', 'Joy', 'Fear', 'Anger', 'Surprise', 'Disgust']\n",
    "\n",
    "# Counting the number of sentences for each emotion\n",
    "emotion_sentences_counts = {emotion: 0 for emotion in emotions}\n",
    "\n",
    "# Loop through each even-numbered column in the DataFrame\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "    # Remove \"Sentences\" from the column name\n",
    "    emotion_name = col_name.replace(\" Sentences\", \"\")\n",
    "    # Check if the column contains any emotions to count\n",
    "    column_emotions = []\n",
    "    for emotion in emotions:\n",
    "        if emotion in emotion_name:\n",
    "            column_emotions.append(emotion)\n",
    "    # If the column contains at least one emotion to count, add the number of non-empty sentences to the count\n",
    "    if len(column_emotions) > 0:\n",
    "        non_empty_sentences = df[col_name].dropna().count()\n",
    "        for emotion in column_emotions:\n",
    "            emotion_sentences_counts[emotion] += non_empty_sentences\n",
    "        # Print the number of non-empty sentences for each emotion and column\n",
    "        print(f\"Column '{col_name}' has {non_empty_sentences} non-empty sentences.\")\n",
    "        for emotion in column_emotions:\n",
    "            print(f\" {non_empty_sentences} sentences added to '{emotion}' count\")\n",
    "\n",
    "# Get total count of sentences in \"Sadness\" + \"Joy\"\n",
    "total_count = emotion_sentences_counts['Sadness'] + emotion_sentences_counts['Joy']\n",
    "# Store our lexicons and sentences\n",
    "emotions_lexicon_bag_of_words = {emotion: [] for emotion in emotions}\n",
    "emotions_sentences_bag_of_words = {emotion: [] for emotion in emotions}\n",
    "\n",
    "# Loop through each emotion\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "\n",
    "    # Extract the emotion name from the column name\n",
    "    emotion = col_name.replace(' Sentences', '')\n",
    "    # Get the index of the \"Sentences\" column for this emotion\n",
    "    sentences_col_idx = df.columns.get_loc(col_name)\n",
    "    # Get emotion lexicon from the \"Lexicon\" column\n",
    "    lexicon = df.iloc[0:, sentences_col_idx-1].tolist()\n",
    "    # Get the sentences from the \"Sentences\" column\n",
    "    sentences = df.iloc[0:, sentences_col_idx].tolist()\n",
    "    # Create an empty list to store the lexicon\n",
    "    lex_tokens = []\n",
    "    # Create an empty list to store the sentences tokens\n",
    "    sentences_tokens = []\n",
    "    # Loop through each lexicon col\n",
    "    for lexeme in lexicon:\n",
    "        # Check if the lexicon is a valid string\n",
    "        if isinstance(lexeme, str):\n",
    "            # Tokenize the words in the each row \"lexeme\"\n",
    "            tokens = nltk.word_tokenize(lexeme)\n",
    "            # Remove commas\n",
    "            tokens_without_commas = [token for token in tokens if token != ',']\n",
    "            # Append to array\n",
    "            lex_tokens.extend(tokens_without_commas)\n",
    "\n",
    "    # Set to lowercase and remove duplicates lexicon\n",
    "    lex_tokens = list(set(token.lower() for token in lex_tokens))\n",
    "    # Loop through each sentence col\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence is a valid string\n",
    "        if isinstance(sentence, str):\n",
    "          # Tokenize the words in each row \"sentence\"\n",
    "          tokens = nltk.word_tokenize(sentence)\n",
    "          # Append to array\n",
    "          sentences_tokens.append(tokens)\n",
    "    # If the emotion contains more than one emotion, split it and add the words to the respective index arrays\n",
    "    if '+' in emotion:\n",
    "        sub_emotions = emotion.split(' + ')\n",
    "        for sub_emotion in sub_emotions:\n",
    "            emotions_lexicon_bag_of_words[sub_emotion].extend(lex_tokens)\n",
    "            emotions_sentences_bag_of_words[sub_emotion].extend(sentences_tokens)\n",
    "    else:\n",
    "        emotions_lexicon_bag_of_words[emotion].extend(lex_tokens)\n",
    "        emotions_sentences_bag_of_words[emotion].extend(sentences_tokens)\n",
    "\n",
    "# Remove duplicates from \"Sadness\" and \"Joy\" lexicon\n",
    "emotions_lexicon_bag_of_words['Sadness'] = list(set(emotions_lexicon_bag_of_words['Sadness']))\n",
    "emotions_lexicon_bag_of_words['Joy'] = list(set(emotions_lexicon_bag_of_words['Joy']))\n",
    "# Define the ground truth labels\n",
    "y_true = [1] * 29 + [0] * 29\n",
    "# Define the predicted labels\n",
    "y_pred = []\n",
    "\n",
    "print(\"\\nPOSITIVE SENTENCES\\n\")\n",
    "for sentence in emotions_sentences_bag_of_words['Joy']:\n",
    "    frequency_distribution = nltk.FreqDist(sentence)\n",
    "    positive_count = sum(frequency_distribution[word] for word in emotions_lexicon_bag_of_words['Joy'])\n",
    "    negative_count = sum(frequency_distribution[word] for word in emotions_lexicon_bag_of_words['Sadness'])\n",
    "    sentence_length = math.log(len(sentence))\n",
    "    w = [0.2325, 0.235, 0.01023586]\n",
    "    b = 0.0025\n",
    "    probability = logistic_regression_prob(positive_count, negative_count, sentence_length, w, b)\n",
    "    if(probability > 0.65):\n",
    "      y_pred.append(1)\n",
    "    else:\n",
    "      y_pred.append(0)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Length:\", sentence_length)\n",
    "    print(\"Positive Count:\", positive_count)\n",
    "    print(\"Negative Count:\", negative_count)\n",
    "    print(\"Probability:\", probability)\n",
    "    print(\"---------\")\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "print(\"\\nNEGATIVE SENTENCES\\n\")\n",
    "for sentence in emotions_sentences_bag_of_words['Sadness']:\n",
    "    frequency_distribution = nltk.FreqDist(sentence)\n",
    "    positive_count = sum(frequency_distribution[word] for word in emotions_lexicon_bag_of_words['Joy'])\n",
    "    negative_count = sum(frequency_distribution[word] for word in emotions_lexicon_bag_of_words['Sadness'])\n",
    "    sentence_length = math.log(len(sentence))\n",
    "    # Weights vector\n",
    "    w = [0.2325, 0.235, 0.01023586]\n",
    "    b = 0.0025\n",
    "    probability = logistic_regression_prob(negative_count, positive_count, sentence_length, w, b)\n",
    "    if(probability > 0.65):\n",
    "      y_pred.append(0)\n",
    "    else:\n",
    "      y_pred.append(1)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Length:\", sentence_length)\n",
    "    print(\"Positive Count:\", positive_count)\n",
    "    print(\"Negative Count:\", negative_count)\n",
    "    print(\"Probability:\", probability)\n",
    "    print(\"---------\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the individual elements of the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute evaluation metrics based on the confusion matrix\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG8sjIq5EPjH",
    "outputId": "9bebd29b-e091-4183-98ee-811ba9e3d02a",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:09:37.470692Z",
     "start_time": "2023-12-22T13:09:37.430408Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Sadness Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Sadness' count\n",
      "Column 'Joy Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Joy' count\n",
      "Column 'Fear Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Fear' count\n",
      "Column 'Anger Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Anger' count\n",
      "Column 'Surprise Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Surprise' count\n",
      "Column 'Disgust Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Sadness' count\n",
      " 10 sentences added to 'Joy' count\n",
      "Column 'Fear + Anger Sentences' has 10 non-empty sentences.\n",
      " 10 sentences added to 'Fear' count\n",
      " 10 sentences added to 'Anger' count\n",
      "Column 'Surprise + Disgust Sentences' has 9 non-empty sentences.\n",
      " 9 sentences added to 'Surprise' count\n",
      " 9 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy + Fear Sentences' has 9 non-empty sentences.\n",
      " 9 sentences added to 'Sadness' count\n",
      " 9 sentences added to 'Joy' count\n",
      " 9 sentences added to 'Fear' count\n",
      "\n",
      "POSITIVE SENTENCES\n",
      "\n",
      "Sentence: ['As', 'a', 'young', 'person', ',', 'I', 'always', 'wonder', 'where', 'my', 'life', 'journey', 'would', 'take', 'me', ',', 'but', 'I', 'hope', 'I', 'gain', 'pleasant', 'experiences', 'along', 'the', 'way', '.', 'I', 'feel', 'blessed', 'and', 'accomplished', 'that', 'I', 'am', 'almost', 'done', 'with', 'my', 'Bachelors', 'degree', 'in', 'Computer', 'Science', '.']\n",
      "Length: 3.8066624897703196\n",
      "Positive Count: 6\n",
      "Negative Count: 2\n",
      "Probability: 0.8706214275367046\n",
      "---------\n",
      "Sentence: ['The', 'elegant', 'party', 'was', 'filled', 'with', 'exquisite', 'decorations', 'and', 'delightful', 'laughter', ',', 'lifting', 'everyone', '’', 's', 'spirit', '.']\n",
      "Length: 2.8903717578961645\n",
      "Positive Count: 2\n",
      "Negative Count: 0\n",
      "Probability: 0.6217741548732681\n",
      "---------\n",
      "Sentence: ['John', 'achieved', 'his', 'lifelong', 'goal', 'of', 'getting', 'the', 'most', 'prestigious', 'award', 'for', 'one', 'of', 'his', 'art', 'pieces', '.']\n",
      "Length: 2.8903717578961645\n",
      "Positive Count: 2\n",
      "Negative Count: 1\n",
      "Probability: 0.6752627412466011\n",
      "---------\n",
      "Sentence: ['The', 'music', 'that', 'was', 'playing', 'in', 'the', 'cafe', 'was', 'as', 'delightful', 'as', 'a', 'sunny', ',', 'afternoon', 'picnic', 'with', 'close', 'friends', '.']\n",
      "Length: 3.044522437723423\n",
      "Positive Count: 2\n",
      "Negative Count: 1\n",
      "Probability: 0.6756086443955753\n",
      "---------\n",
      "Sentence: ['The', 'moment', 'I', 'laid', 'my', 'eyes', 'on', 'the', 'acceptance', 'letter', 'from', 'my', 'dream', 'school', ',', 'an', 'overwhelming', 'feeling', 'of', 'elation', 'and', 'achievement', 'washed', 'over', 'me', '.']\n",
      "Length: 3.258096538021482\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6232461338859043\n",
      "---------\n",
      "Sentence: ['Pleased', 'with', 'her', 'performance', ',', 'Maggie', 'was', 'able', 'to', 'win', 'the', 'tournament', 'and', 'finally', 'wear', 'the', 'first', 'place', 'medal', 'she', 'longed', 'for', '.']\n",
      "Length: 3.1354942159291497\n",
      "Positive Count: 1\n",
      "Negative Count: 0\n",
      "Probability: 0.5663794654226085\n",
      "---------\n",
      "Sentence: ['The', 'pride', 'of', 'their', 'accomplishment', 'welled', 'up', 'within', 'their', 'hearts', 'as', 'the', 'crowd', 'cheered', 'on', '.']\n",
      "Length: 2.772588722239781\n",
      "Positive Count: 2\n",
      "Negative Count: 0\n",
      "Probability: 0.6214905885447458\n",
      "---------\n",
      "Sentence: ['I', 'was', 'grateful', 'to', 'have', 'met', 'him', ',', 'his', 'youthful', 'attitude', 'helped', 'me', 'reinvigorate', 'my', 'own', 'zest', 'for', 'life', '.']\n",
      "Length: 2.995732273553991\n",
      "Positive Count: 0\n",
      "Negative Count: 0\n",
      "Probability: 0.5082902142227865\n",
      "---------\n",
      "Sentence: ['The', 'father', 'looked', 'at', 'his', 'son', '’', 's', 'youthful', 'enjoyment', 'as', 'he', 'won', 'the', 'basketball', 'game', 'with', 'his', 'teammates', '.']\n",
      "Length: 2.995732273553991\n",
      "Positive Count: 0\n",
      "Negative Count: 0\n",
      "Probability: 0.5082902142227865\n",
      "---------\n",
      "Sentence: ['Loving', ',', 'passionate', 'sex', 'always', 'ends', 'with', 'a', 'magical', 'orgasm', '.']\n",
      "Length: 2.3978952727983707\n",
      "Positive Count: 4\n",
      "Negative Count: 0\n",
      "Probability: 0.7225296788153307\n",
      "---------\n",
      "Sentence: ['I', 'felt', 'betrayed', 'that', 'my', 'favorite', 'company', 'disqualified', 'me', 'after', 'the', 'interview', 'process', ',', 'that', 'made', 'me', 'feel', 'so', 'humiliated', 'having', 'to', 'apply', 'to', 'other', 'companies', ',', 'however', 'I', 'am', 'grateful', 'to', 'get', 'hired', 'in', 'a', 'prestigious', 'company', 'and', 'I', '’', 'm', 'blessed', 'to', 'be', 'making', 'excellent', 'progress', '.']\n",
      "Length: 3.8918202981106265\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8092024365687904\n",
      "---------\n",
      "Sentence: ['The', 'artist', 'felt', 'a', 'strange', 'mix', 'of', 'sadness', 'and', 'joy', 'as', 'she', 'restored', 'the', 'desecrated', 'painting', ',', 'knowing', 'the', 'was', 'reviving', 'the', 'masterpiece', 'for', 'future', 'connoisseurs', 'to', 'appreciate', '.']\n",
      "Length: 3.367295829986474\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6235085567080946\n",
      "---------\n",
      "Sentence: ['We', 'commemorated', 'the', 'closure', 'of', 'our', 'store', 'by', 'leaving', 'a', 'time', 'capsule', 'buried', 'somewhere', 'inside', '.']\n",
      "Length: 2.772588722239781\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6220785097635373\n",
      "---------\n",
      "Sentence: ['The', 'sonnet', 'the', 'man', 'made', 'about', 'his', 'mother', 'who', 'had', 'recently', 'passed', 'was', 'so', 'lovely', 'that', 'it', 'had', 'made', 'it', 'onto', 'the', 'newspaper', '.']\n",
      "Length: 3.1780538303479458\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8080718840403682\n",
      "---------\n",
      "Sentence: ['We', 'felt', 'melancholic', 'as', 'we', 'watched', 'the', 'beautiful', 'sunset', 'over', 'the', 'ocean', ',', 'reflecting', 'on', 'the', 'beauty', 'of', 'the', 'moment', 'and', 'the', 'fleeting', 'nature', 'of', 'life', '.']\n",
      "Length: 3.295836866004329\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7253657354118679\n",
      "---------\n",
      "Sentence: ['Though', 'Sandra', 'was', 'thrilled', 'to', 'be', 'finally', 'graduating', 'after', '4', 'years', ',', 'she', 'could', 'not', 'help', 'but', 'be', 'worried', 'about', 'entering', 'the', 'job', 'market', '.']\n",
      "Length: 3.2188758248682006\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.62315186277672\n",
      "---------\n",
      "Sentence: ['Her', 'heart', 'brimmed', 'so', 'fully', 'with', 'hopes', 'and', 'dreams', 'it', '’', 's', 'a', 'wonder', 'there', 'was', 'room', 'for', 'the', 'pain', '.']\n",
      "Length: 3.044522437723423\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6227326733280463\n",
      "---------\n",
      "Sentence: ['The', 'abandonment', 'she', 'had', 'went', 'through', 'made', 'her', 'realize', 'that', 'she', 'needed', 'to', 'find', 'her', 'own', 'youth', 'and', 'zest', 'again', '.']\n",
      "Length: 3.044522437723423\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7248529860523452\n",
      "---------\n",
      "Sentence: ['George', 'flashed', 'a', 'heartfelt', 'smiled', 'as', 'he', 'listened', 'to', 'his', 'favorite', 'music', 'for', 'the', 'final', 'time', 'before', 'going', 'deaf', '.']\n",
      "Length: 2.995732273553991\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7247533721129785\n",
      "---------\n",
      "Sentence: ['It', '’', 's', 'reclaiming', '–', 'to', 'build', 'art', 'and', 'music', 'from', 'your', 'pain', '.']\n",
      "Length: 2.6390573296152584\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8072147740498917\n",
      "---------\n",
      "Sentence: ['After', 'news', 'of', 'you', 'being', 'unwell', ',', 'putting', 'you', 'down', 'was', 'the', 'only', 'way', 'to', 'end', 'your', 'pain', ',', 'but', 'I', 'did', \"n't\", 'want', 'to', 'risk', 'losing', 'my', 'childhood', 'friend', '.', 'In', 'my', 'memories', ',', 'you', 'and', 'I', 'were', 'inseparable', 'that', 'the', 'thought', 'of', 'your', 'absence', 'became', 'unbearable', ',', 'but', 'I', 'only', 'hope', 'that', 'you', 'were', 'happy', 'until', 'the', 'very', 'end', ',', 'even', 'in', 'heaven', '.']\n",
      "Length: 4.189654742026425\n",
      "Positive Count: 10\n",
      "Negative Count: 10\n",
      "Probability: 0.9911669683976967\n",
      "---------\n",
      "Sentence: ['As', 'the', 'shepherd', 'watches', 'the', 'young', 'lamb', 'escape', 'the', 'whirlpool', ',', 'he', 'felt', 'a', 'mix', 'of', 'sadness', ',', 'joy', ',', 'and', 'fear', ',', 'pensive', 'about', 'the', 'risks', 'his', 'beloved', 'animals', 'faced', 'in', 'the', 'unpredictable', 'wilderness', '.']\n",
      "Length: 3.58351893845611\n",
      "Positive Count: 5\n",
      "Negative Count: 4\n",
      "Probability: 0.894888778440514\n",
      "---------\n",
      "Sentence: ['We', 'were', 'about', 'to', 'reach', 'our', 'final', 'destination', '.', 'To', 'be', 'honest', ',', 'I', 'wish', 'that', 'our', 'journey', 'could', 'be', 'endless', '.', 'I', 'have', 'a', 'feeling', 'that', 'we', 'may', 'not', 'see', 'each', 'other']\n",
      "Length: 3.4965075614664802\n",
      "Positive Count: 5\n",
      "Negative Count: 4\n",
      "Probability: 0.8948049731830062\n",
      "---------\n",
      "Sentence: ['The', 'outburst', 'from', 'the', 'woman', 'made', 'all', 'of', 'the', 'immediate', 'onlookers', 'sympathetic', 'to', 'her', 'plight', ',', 'due', 'to', 'her', 'raw', ',', 'honest', 'emotions', '.']\n",
      "Length: 3.1780538303479458\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8080718840403682\n",
      "---------\n",
      "Sentence: ['Link', '’', 's', 'quest', 'to', 'save', 'Hyrule', 'from', 'calamity', 'Gannon', 'and', 'rescue', 'Princess', 'Zelda', 'from', 'abduction', 'was', 'filled', 'with', 'heart-wrenching', 'sadness', 'and', 'triumphant', 'joy', 'as', 'he', 'battled', 'against', 'the', 'forces', 'of', 'evil', 'to', 'restore', 'peace', 'to', 'the', 'land', '.']\n",
      "Length: 3.6635616461296463\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8088414465005185\n",
      "---------\n",
      "Sentence: ['Though', 'he', 'was', 'rejoicing', 'that', 'the', 'stab', 'wound', 'looked', 'like', 'it', 'was', 'healing', 'nicely', ',', 'he', 'was', 'weary', 'that', 'there', 'was', 'an', 'unknown', 'infection', '.']\n",
      "Length: 3.2188758248682006\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7252087771735315\n",
      "---------\n",
      "Sentence: ['I', 'worry', 'that', 'someday', 'I', '’', 'll', 'forget', 'the', 'love', 'I', '’', 've', 'lost', '.']\n",
      "Length: 2.70805020110221\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7241655626195512\n",
      "---------\n",
      "Sentence: ['She', 'yelled', 'urgently', 'before', 'I', 'was', 'able', 'to', 'leave', ',', 'hoping', 'to', 'fin', 'absolution', 'from', 'her', 'wrongdoings', '.']\n",
      "Length: 2.8903717578961645\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6223619032737497\n",
      "---------\n",
      "Sentence: ['As', 'Ray', 'waved', 'goodbye', 'to', 'his', 'daughter', 'Ashley', 'going', 'off', 'to', 'college', ',', 'a', 'weight', 'was', 'removed', 'from', 'his', 'shoulders', 'as', 'he', 'knew', 'there', 'were', 'endless', 'possibilities', 'at', 'this', 'new', 'destination', ',', 'and', 'Ashley', 'was', 'ready', 'to', 'experience', 'it', 'all', '.']\n",
      "Length: 3.713572066704308\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8089205823580159\n",
      "---------\n",
      "------------------------------------------\n",
      "\n",
      "NEGATIVE SENTENCES\n",
      "\n",
      "Sentence: ['Two', 'years', 'ago', ',', 'my', 'friend', 'abandoned', 'her', 'dog', 'in', 'the', 'park', 'and', 'she', 'still', 'regrets', 'her', 'wrongful', 'action', '.', 'I', 'got', 'so', 'disappointed', 'that', 'Bank', 'of', 'America', 'had', 'disabled', 'my', 'credit', 'account', 'due', 'to', 'an', 'unpaid', 'balance', '.']\n",
      "Length: 3.6635616461296463\n",
      "Positive Count: 2\n",
      "Negative Count: 5\n",
      "Probability: 0.8419088121075038\n",
      "---------\n",
      "Sentence: ['He', 'sat', 'alone', 'by', 'the', 'window', ',', 'lost', 'in', 'a', 'pensive', 'mood', 'as', 'he', 'reminisced', 'about', 'the', 'past', '.']\n",
      "Length: 2.9444389791664403\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6224919644771412\n",
      "---------\n",
      "Sentence: ['Tim', 'was', 'unlucky', '.', 'He', 'had', 'a', 'traumatic', 'experience', 'witnessing', 'the', 'slaughtering', 'of', 'captive', 'prisoners', '.', 'His', 'horrific', 'experience', 'made', 'him', 'resign', 'from', 'the', 'military', 'and', 'put', 'himself', 'into', 'isolation', '.']\n",
      "Length: 3.4339872044851463\n",
      "Positive Count: 0\n",
      "Negative Count: 6\n",
      "Probability: 0.8073138514809377\n",
      "---------\n",
      "Sentence: ['After', 'his', 'wife', 'left', 'him', ',', 'he', 'lost', 'his', 'job', ',', 'and', 'his', 'dog', 'passed', ',', 'he', 'was', 'resigned', 'to', 'a', 'life', 'of', 'eternal', 'anguish', '.']\n",
      "Length: 3.258096538021482\n",
      "Positive Count: 0\n",
      "Negative Count: 2\n",
      "Probability: 0.6226589272881781\n",
      "---------\n",
      "Sentence: ['The', 'news', 'caused', 'my', 'world', 'to', 'collapse', ',', 'leaving', 'me', 'feeling', 'lost', 'and', 'hopeless', '.']\n",
      "Length: 2.70805020110221\n",
      "Positive Count: 1\n",
      "Negative Count: 2\n",
      "Probability: 0.6748533776581092\n",
      "---------\n",
      "Sentence: ['He', 'was', 'distraught', 'by', 'the', 'horrors', 'he', 'saw', 'during', 'his', 'time', 'in', 'the', 'war', '.']\n",
      "Length: 2.70805020110221\n",
      "Positive Count: 0\n",
      "Negative Count: 1\n",
      "Probability: 0.5653046191032777\n",
      "---------\n",
      "Sentence: ['Grief', ',', 'I', '’', 've', 'learned', ',', 'is', 'really', 'just', 'love', '.', 'It', '’', 's', 'all', 'the', 'love', 'you', 'want', 'to', 'give', ',', 'but', 'can', 'not', '.', 'All', 'that', 'unspent', 'love', 'gathers', 'up', 'in', 'the', 'corners', 'of', 'your', 'eyes', ',', 'the', 'lump', 'in', 'your', 'throat', ',', 'and', 'in', 'that', 'hollow', 'part', 'of', 'your', 'chest', '.', 'Grief', 'is', 'just', 'love', 'with', 'no', 'place', 'to', 'go', '.']\n",
      "Length: 4.174387269895637\n",
      "Positive Count: 4\n",
      "Negative Count: 5\n",
      "Probability: 0.895456317668121\n",
      "---------\n",
      "Sentence: ['She', 'abandoned', 'me', 'last', 'night', 'after', 'saying', 'some', 'wretched', 'things', 'to', 'me', 'at', 'the', 'party', '.']\n",
      "Length: 2.772588722239781\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7242974992466749\n",
      "---------\n",
      "Sentence: ['Richard', 'lowered', 'his', 'head', 'in', 'disbelief', 'as', 'he', 'was', 'wrongfully', 'accused', 'of', 'robbing', 'the', 'bank', '.']\n",
      "Length: 2.772588722239781\n",
      "Positive Count: 0\n",
      "Negative Count: 0\n",
      "Probability: 0.507719344102032\n",
      "---------\n",
      "Sentence: ['The', 'somber', 'sycophant', 'whimpers', 'and', 'weeps', 'in', 'their', 'sterile', 'house', ',', 'unable', 'to', 'break', 'their', 'bonds', 'to', 'flattery', '.']\n",
      "Length: 2.9444389791664403\n",
      "Positive Count: 0\n",
      "Negative Count: 2\n",
      "Probability: 0.6219042955248113\n",
      "---------\n",
      "Sentence: ['I', 'felt', 'betrayed', 'that', 'my', 'favorite', 'company', 'disqualified', 'me', 'after', 'the', 'interview', 'process', ',', 'that', 'made', 'me', 'feel', 'so', 'humiliated', 'having', 'to', 'apply', 'to', 'other', 'companies', ',', 'however', 'I', 'am', 'grateful', 'to', 'get', 'hired', 'in', 'a', 'prestigious', 'company', 'and', 'I', '’', 'm', 'blessed', 'to', 'be', 'making', 'excellent', 'progress', '.']\n",
      "Length: 3.8918202981106265\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8092024365687904\n",
      "---------\n",
      "Sentence: ['The', 'artist', 'felt', 'a', 'strange', 'mix', 'of', 'sadness', 'and', 'joy', 'as', 'she', 'restored', 'the', 'desecrated', 'painting', ',', 'knowing', 'the', 'was', 'reviving', 'the', 'masterpiece', 'for', 'future', 'connoisseurs', 'to', 'appreciate', '.']\n",
      "Length: 3.367295829986474\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6235085567080946\n",
      "---------\n",
      "Sentence: ['We', 'commemorated', 'the', 'closure', 'of', 'our', 'store', 'by', 'leaving', 'a', 'time', 'capsule', 'buried', 'somewhere', 'inside', '.']\n",
      "Length: 2.772588722239781\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6220785097635373\n",
      "---------\n",
      "Sentence: ['The', 'sonnet', 'the', 'man', 'made', 'about', 'his', 'mother', 'who', 'had', 'recently', 'passed', 'was', 'so', 'lovely', 'that', 'it', 'had', 'made', 'it', 'onto', 'the', 'newspaper', '.']\n",
      "Length: 3.1780538303479458\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8080718840403682\n",
      "---------\n",
      "Sentence: ['We', 'felt', 'melancholic', 'as', 'we', 'watched', 'the', 'beautiful', 'sunset', 'over', 'the', 'ocean', ',', 'reflecting', 'on', 'the', 'beauty', 'of', 'the', 'moment', 'and', 'the', 'fleeting', 'nature', 'of', 'life', '.']\n",
      "Length: 3.295836866004329\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7253657354118679\n",
      "---------\n",
      "Sentence: ['Though', 'Sandra', 'was', 'thrilled', 'to', 'be', 'finally', 'graduating', 'after', '4', 'years', ',', 'she', 'could', 'not', 'help', 'but', 'be', 'worried', 'about', 'entering', 'the', 'job', 'market', '.']\n",
      "Length: 3.2188758248682006\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.62315186277672\n",
      "---------\n",
      "Sentence: ['Her', 'heart', 'brimmed', 'so', 'fully', 'with', 'hopes', 'and', 'dreams', 'it', '’', 's', 'a', 'wonder', 'there', 'was', 'room', 'for', 'the', 'pain', '.']\n",
      "Length: 3.044522437723423\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6227326733280463\n",
      "---------\n",
      "Sentence: ['The', 'abandonment', 'she', 'had', 'went', 'through', 'made', 'her', 'realize', 'that', 'she', 'needed', 'to', 'find', 'her', 'own', 'youth', 'and', 'zest', 'again', '.']\n",
      "Length: 3.044522437723423\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7248529860523452\n",
      "---------\n",
      "Sentence: ['George', 'flashed', 'a', 'heartfelt', 'smiled', 'as', 'he', 'listened', 'to', 'his', 'favorite', 'music', 'for', 'the', 'final', 'time', 'before', 'going', 'deaf', '.']\n",
      "Length: 2.995732273553991\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7247533721129785\n",
      "---------\n",
      "Sentence: ['It', '’', 's', 'reclaiming', '–', 'to', 'build', 'art', 'and', 'music', 'from', 'your', 'pain', '.']\n",
      "Length: 2.6390573296152584\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8072147740498917\n",
      "---------\n",
      "Sentence: ['After', 'news', 'of', 'you', 'being', 'unwell', ',', 'putting', 'you', 'down', 'was', 'the', 'only', 'way', 'to', 'end', 'your', 'pain', ',', 'but', 'I', 'did', \"n't\", 'want', 'to', 'risk', 'losing', 'my', 'childhood', 'friend', '.', 'In', 'my', 'memories', ',', 'you', 'and', 'I', 'were', 'inseparable', 'that', 'the', 'thought', 'of', 'your', 'absence', 'became', 'unbearable', ',', 'but', 'I', 'only', 'hope', 'that', 'you', 'were', 'happy', 'until', 'the', 'very', 'end', ',', 'even', 'in', 'heaven', '.']\n",
      "Length: 4.189654742026425\n",
      "Positive Count: 10\n",
      "Negative Count: 10\n",
      "Probability: 0.9911669683976967\n",
      "---------\n",
      "Sentence: ['As', 'the', 'shepherd', 'watches', 'the', 'young', 'lamb', 'escape', 'the', 'whirlpool', ',', 'he', 'felt', 'a', 'mix', 'of', 'sadness', ',', 'joy', ',', 'and', 'fear', ',', 'pensive', 'about', 'the', 'risks', 'his', 'beloved', 'animals', 'faced', 'in', 'the', 'unpredictable', 'wilderness', '.']\n",
      "Length: 3.58351893845611\n",
      "Positive Count: 5\n",
      "Negative Count: 4\n",
      "Probability: 0.8951237035266106\n",
      "---------\n",
      "Sentence: ['We', 'were', 'about', 'to', 'reach', 'our', 'final', 'destination', '.', 'To', 'be', 'honest', ',', 'I', 'wish', 'that', 'our', 'journey', 'could', 'be', 'endless', '.', 'I', 'have', 'a', 'feeling', 'that', 'we', 'may', 'not', 'see', 'each', 'other']\n",
      "Length: 3.4965075614664802\n",
      "Positive Count: 5\n",
      "Negative Count: 4\n",
      "Probability: 0.8950400636062675\n",
      "---------\n",
      "Sentence: ['The', 'outburst', 'from', 'the', 'woman', 'made', 'all', 'of', 'the', 'immediate', 'onlookers', 'sympathetic', 'to', 'her', 'plight', ',', 'due', 'to', 'her', 'raw', ',', 'honest', 'emotions', '.']\n",
      "Length: 3.1780538303479458\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8080718840403682\n",
      "---------\n",
      "Sentence: ['Link', '’', 's', 'quest', 'to', 'save', 'Hyrule', 'from', 'calamity', 'Gannon', 'and', 'rescue', 'Princess', 'Zelda', 'from', 'abduction', 'was', 'filled', 'with', 'heart-wrenching', 'sadness', 'and', 'triumphant', 'joy', 'as', 'he', 'battled', 'against', 'the', 'forces', 'of', 'evil', 'to', 'restore', 'peace', 'to', 'the', 'land', '.']\n",
      "Length: 3.6635616461296463\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8088414465005185\n",
      "---------\n",
      "Sentence: ['Though', 'he', 'was', 'rejoicing', 'that', 'the', 'stab', 'wound', 'looked', 'like', 'it', 'was', 'healing', 'nicely', ',', 'he', 'was', 'weary', 'that', 'there', 'was', 'an', 'unknown', 'infection', '.']\n",
      "Length: 3.2188758248682006\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7252087771735315\n",
      "---------\n",
      "Sentence: ['I', 'worry', 'that', 'someday', 'I', '’', 'll', 'forget', 'the', 'love', 'I', '’', 've', 'lost', '.']\n",
      "Length: 2.70805020110221\n",
      "Positive Count: 2\n",
      "Negative Count: 2\n",
      "Probability: 0.7241655626195512\n",
      "---------\n",
      "Sentence: ['She', 'yelled', 'urgently', 'before', 'I', 'was', 'able', 'to', 'leave', ',', 'hoping', 'to', 'fin', 'absolution', 'from', 'her', 'wrongdoings', '.']\n",
      "Length: 2.8903717578961645\n",
      "Positive Count: 1\n",
      "Negative Count: 1\n",
      "Probability: 0.6223619032737497\n",
      "---------\n",
      "Sentence: ['As', 'Ray', 'waved', 'goodbye', 'to', 'his', 'daughter', 'Ashley', 'going', 'off', 'to', 'college', ',', 'a', 'weight', 'was', 'removed', 'from', 'his', 'shoulders', 'as', 'he', 'knew', 'there', 'were', 'endless', 'possibilities', 'at', 'this', 'new', 'destination', ',', 'and', 'Ashley', 'was', 'ready', 'to', 'experience', 'it', 'all', '.']\n",
      "Length: 3.713572066704308\n",
      "Positive Count: 3\n",
      "Negative Count: 3\n",
      "Probability: 0.8089205823580159\n",
      "---------\n",
      "Confusion Matrix:\n",
      "[[19 10]\n",
      " [11 18]]\n",
      "Accuracy: 0.6379310344827587\n",
      "Precision: 0.6428571428571429\n",
      "Recall: 0.6206896551724138\n",
      "F1 Score: 0.6315789473684211\n"
     ]
    }
   ]
  }
 ]
}
