{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Emotion Classification Using NaÃ¯ve Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fnmctaA195D",
    "outputId": "46609004-0b0e-4e11-cca2-8c3e559d5322",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:01:33.299641Z",
     "start_time": "2023-12-22T13:01:32.532604Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C29Q7BtUytCE",
    "ExecuteTime": {
     "end_time": "2023-12-22T13:03:01.655056Z",
     "start_time": "2023-12-22T13:03:01.581748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadness: ['The', 'devastating', 'news', 'of', 'the', 'child', \"'s\", 'abduction', 'left', 'a', 'solemn', 'shadow', 'over', 'the', 'family', 'for', 'the', 'next', 'month', '.']\n",
      "Joy: ['It', 'was', 'a', 'sunny', 'summer', 'morning', 'and', 'the', 'laughter', 'of', 'children', 'could', 'be', 'heard', 'from', 'the', 'pool', 'as', 'they', 'splashed', 'water', 'onto', 'each', 'other', '.']\n",
      "Fear: ['As', 'he', 'walked', 'in', 'the', 'dead', 'of', 'night', 'he', 'could', 'hear', 'sudden', 'footsteps', 'echoing', 'from', 'the', 'alleyway', ',', 'a', 'shiver', 'went', 'down', 'his', 'spine', '.']\n",
      "Anger: ['While', 'driving', 'his', 'family', 'to', 'a', 'restaurant', 'a', 'car', 'recklessly', 'changed', 'lanes', 'barely', 'missing', 'him', ',', 'his', 'face', 'flushed', 'red', 'with', 'fury', 'knowing', 'how', 'close', 'his', 'family', 'was', 'to', 'being', 'seriously', 'harmed', '.']\n",
      "Surprise: ['She', 'was', 'startled', 'unexpectedly', 'as', 'everyone', 'sprang', 'from', 'their', 'hiding', 'spot', 'to', 'celebrate', 'her', 'birthday']\n",
      "Disgust: ['She', 'had', 'forgotten', 'to', 'take', 'out', 'the', 'trash', 'before', 'leaving', 'for', 'holiday', ',', 'when', 'she', 'returned', 'she', 'was', 'repulsed', 'by', 'the', 'stench', 'of', 'decaying', 'food', '.']\n",
      "Column 'Sadness Sentences' has 30 sentences.\n",
      "Column 'Joy Sentences' has 30 sentences.\n",
      "Column 'Fear Sentences' has 30 sentences.\n",
      "Column 'Anger Sentences' has 30 sentences.\n",
      "Column 'Surprise Sentences' has 29 sentences.\n",
      "Column 'Disgust Sentences' has 30 sentences.\n",
      "Column 'Sadness + Joy Sentences' has 30 sentences.\n",
      "Column 'Fear + Anger Sentences' has 30 sentences.\n",
      "Column 'Surprise + Disgust Sentences' has 28 sentences.\n",
      "Column 'Sadness + Joy + Fear Sentences' has 29 sentences.\n",
      "Column 'Sadness Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Sadness' count\n",
      "Column 'Joy Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Joy' count\n",
      "Column 'Fear Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Fear' count\n",
      "Column 'Anger Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Anger' count\n",
      "Column 'Surprise Sentences' has 29 non-empty sentences.\n",
      " 29 sentences added to 'Surprise' count\n",
      "Column 'Disgust Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Sadness' count\n",
      " 30 sentences added to 'Joy' count\n",
      "Column 'Fear + Anger Sentences' has 30 non-empty sentences.\n",
      " 30 sentences added to 'Fear' count\n",
      " 30 sentences added to 'Anger' count\n",
      "Column 'Surprise + Disgust Sentences' has 28 non-empty sentences.\n",
      " 28 sentences added to 'Surprise' count\n",
      " 28 sentences added to 'Disgust' count\n",
      "Column 'Sadness + Joy + Fear Sentences' has 29 non-empty sentences.\n",
      " 29 sentences added to 'Sadness' count\n",
      " 29 sentences added to 'Joy' count\n",
      " 29 sentences added to 'Fear' count\n",
      "Total number of sentences (that belongs to 6 emotions): 442\n",
      "P(Sadness)= 89 / 442 = 0.20135746606334842\n",
      "P(Joy)= 89 / 442 = 0.20135746606334842\n",
      "P(Fear)= 89 / 442 = 0.20135746606334842\n",
      "P(Anger)= 60 / 442 = 0.13574660633484162\n",
      "P(Surprise)= 57 / 442 = 0.12895927601809956\n",
      "P(Disgust)= 58 / 442 = 0.13122171945701358\n",
      "Total probability: 1.0\n",
      "Normalized posterior probabilities for each emotion:\n",
      "\n",
      "P(Sadness|S) = 8.831686750987629e-17\n",
      "P(Joy|S) = 8.724298157289628e-09\n",
      "P(Fear|S) = 1.57323958086014e-14\n",
      "P(Anger|S) = 1.0511113988568794e-13\n",
      "P(Surprise|S) = 0.9999289892373084\n",
      "P(Disgust|S) = 7.100203827245855e-05\n",
      "\n",
      "Total Sum Probabilities = 1.0\n",
      "\n",
      "Text: As she hugged her daughter goodbye on the first day of college, she felt both sad to see her go and joyful knowing that she was embarking on a new and exciting chapter in her life.\n",
      "\n",
      "Predicted emotion: Surprise\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel sheet into a pandas DataFrame\n",
    "df = pd.read_excel('Files/data.xlsx')\n",
    "\n",
    "# Select the first 30 rows of the DataFrame\n",
    "df = df.head(30)\n",
    "\n",
    "# Define the emotions to count\n",
    "emotions = ['Sadness', 'Joy', 'Fear', 'Anger', 'Surprise', 'Disgust']\n",
    "\n",
    "# Loop through the first 6 rows in even-numbered columns of the DataFrame\n",
    "for i, col_name in enumerate(df.columns[1::2][:6]):\n",
    "    # Get the sentences from the column\n",
    "    sentences = df[col_name].tolist()\n",
    "    tokens = nltk.word_tokenize(sentences[0])\n",
    "    print(f\"{emotions[i]}: {tokens}\")\n",
    "            \n",
    "\n",
    "# Count the number of total sentences in doc\n",
    "total_num_sentences = 0\n",
    "for col_name in df.columns[1::2]:\n",
    "    # Check how many sentences in that column\n",
    "    num_sentences = 0\n",
    "    for value in df[col_name]:\n",
    "        if not pd.isna(value):\n",
    "            num_sentences += 1\n",
    "    # Print how many sentences that column has\n",
    "    print(f\"Column '{col_name}' has {num_sentences} sentences.\")\n",
    "    # Add number of sentences to total number\n",
    "    total_num_sentences += num_sentences\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the sentence counts for each emotion\n",
    "emotion_counts = {emotion: 0 for emotion in emotions}\n",
    "\n",
    "# Loop through each even-numbered column in the DataFrame\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "    # Remove \"Sentences\" from the column name\n",
    "    emotion_name = col_name.replace(\" Sentences\", \"\")\n",
    "    # Check if the column contains any emotions to count\n",
    "    column_emotions = []\n",
    "    for emotion in emotions:\n",
    "        if emotion in emotion_name:\n",
    "            column_emotions.append(emotion)\n",
    "    # If the column contains at least one emotion to count, add the number of non-empty sentences to the count\n",
    "    if len(column_emotions) > 0:\n",
    "        non_empty_sentences = df[col_name].dropna().count()\n",
    "        for emotion in column_emotions:\n",
    "            emotion_counts[emotion] += non_empty_sentences\n",
    "        # Print the number of non-empty sentences for each emotion and column\n",
    "        print(f\"Column '{col_name}' has {non_empty_sentences} non-empty sentences.\")\n",
    "        for emotion in column_emotions:\n",
    "            print(f\" {non_empty_sentences} sentences added to '{emotion}' count\")\n",
    "\n",
    "total_count = 0\n",
    "# Print the total non-empty sentence counts for each emotion\n",
    "for emotion, count in emotion_counts.items():\n",
    "    total_count += count\n",
    "\n",
    "\n",
    "print(f\"Total number of sentences (that belongs to 6 emotions): {total_count}\")\n",
    "\n",
    "# Initialize a variable to store the sum of all probabilities\n",
    "total_prob = 0\n",
    "\n",
    "# Print the total non-empty sentence counts for each emotion\n",
    "for emotion, count in emotion_counts.items():\n",
    "    prob = count / total_count\n",
    "    total_prob += prob\n",
    "    print(f\"P({emotion})= {count} / {total_count} = {prob}\")\n",
    "\n",
    "# Print the sum of all probabilities\n",
    "print(f\"Total probability: {total_prob}\")\n",
    "\n",
    "\n",
    "# Define the emotions to count\n",
    "emotions = ['Sadness', 'Joy', 'Fear', 'Anger', 'Surprise', 'Disgust']\n",
    "\n",
    "emotion_sentences_count=[]\n",
    "emotions_lexicon_bag_of_words = []\n",
    "emotions_sentences_bag_of_words = []\n",
    "\n",
    "# Loop through each emotion\n",
    "for i, col_name in enumerate(df.columns[1::2]):\n",
    "\n",
    "    # Extract the emotion name from the column name\n",
    "    emotion = col_name.replace(' Sentences', '')\n",
    "\n",
    "    # Get the index of the \"Sentences\" column for this emotion\n",
    "    sentences_col_idx = df.columns.get_loc(col_name)\n",
    "\n",
    "    # Get emotion lexicon from the \"Lexicon\" column\n",
    "    lexicon = df.iloc[0:, sentences_col_idx-1].tolist()\n",
    "\n",
    "    # Get the sentences from the \"Sentences\" column\n",
    "    sentences = df.iloc[0:, sentences_col_idx].tolist()\n",
    "\n",
    "    # Create an empty list to store the lexicon\n",
    "    lex_tokens = []\n",
    "\n",
    "    # Create an empty list to store the sentences tokens\n",
    "    sentences_tokens = []\n",
    "\n",
    "    # Loop through each lexicon col\n",
    "    for lexeme in lexicon:\n",
    "        # Check if the lexicon is a valid string\n",
    "        if isinstance(lexeme, str):\n",
    "            # Tokenize the words in the each row \"lexeme\"\n",
    "            tokens = nltk.word_tokenize(lexeme)\n",
    "            # Remove commas\n",
    "            tokens_without_commas = [token for token in tokens if token != ',']\n",
    "            # Append to array\n",
    "            lex_tokens.extend(tokens_without_commas)\n",
    "\n",
    "    # Remove duplicates lexicon\n",
    "    lex_tokens = list(set(lex_tokens))\n",
    "\n",
    "    # Loop through each sentence col\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence is a valid string\n",
    "        if isinstance(sentence, str):\n",
    "          # Tokenize the words in each row \"sentence\"\n",
    "          tokens = nltk.word_tokenize(sentence)\n",
    "          # Append to array\n",
    "          sentences_tokens.extend(tokens)\n",
    "\n",
    "\n",
    "    # If the emotion contains more than one emotion, split it and add the words to the respective index arrays\n",
    "    if '+' in emotion:\n",
    "        sub_emotions = emotion.split(' + ')\n",
    "        for sub_emotion in sub_emotions:\n",
    "            sub_emotion_index = emotions.index(sub_emotion)\n",
    "            emotions_lexicon_bag_of_words[sub_emotion_index].extend(lex_tokens)\n",
    "            emotions_sentences_bag_of_words[sub_emotion_index].extend(sentences_tokens)\n",
    "    else:\n",
    "        emotion_index = emotions.index(emotion)\n",
    "        emotions_lexicon_bag_of_words.insert(emotion_index, lex_tokens)\n",
    "        emotions_sentences_bag_of_words.insert(emotion_index, sentences_tokens)\n",
    "\n",
    "\n",
    "# Loop through the array of arrays and remove duplicates while converting to lowercase\n",
    "for i, bag in enumerate(emotions_lexicon_bag_of_words):\n",
    "    # Convert each string to lowercase and remove duplicates in place\n",
    "    unique_bag = []\n",
    "    for s in bag:\n",
    "        if s.lower() not in unique_bag:\n",
    "            unique_bag.append(s.lower())\n",
    "    emotions_lexicon_bag_of_words[i] = unique_bag\n",
    "\n",
    "\n",
    "# Set the vocabulary size to the total number of words in all sentences doc\n",
    "vocabulary_size = 0\n",
    "\n",
    "for bag in emotions_sentences_bag_of_words:\n",
    "    vocabulary_size += len(bag)\n",
    "\n",
    "# Initialize a dictionary to store the prior_probabilities for each emotion\n",
    "prior_probabilities = {emotion: {} for emotion in emotions}\n",
    "\n",
    "# Loop through each emotion and their respective tokenized sentences\n",
    "for emotion in emotions:\n",
    "    sentences_words_bag = emotions_sentences_bag_of_words[emotions.index(emotion)]\n",
    "\n",
    "    # Count the number of occurrences of each word\n",
    "    word_counts = FreqDist(sentences_words_bag)\n",
    "\n",
    "    for word, frequency in word_counts.most_common():\n",
    "      # print(f'{word}: {frequency}')\n",
    "      likelihood = (frequency + 1) / (len(sentences_words_bag) + vocabulary_size)\n",
    "      prior_probabilities[emotion][word] = likelihood\n",
    "\n",
    "\n",
    "s_text = \"As she hugged her daughter goodbye on the first day of college, she felt both sad to see her go and joyful knowing that she was embarking on a new and exciting chapter in her life.\"\n",
    "\n",
    "s_tokens = nltk.word_tokenize(s_text)\n",
    "\n",
    "# Initialize a dictionary to store the posterior probabilities for each emotion\n",
    "posteriors_probabilities = {emotion: 1 for emotion in emotions}\n",
    "\n",
    "# Loop through each token in the text\n",
    "for token in s_tokens:\n",
    "    # Loop through each emotion\n",
    "    for emotion in emotions:\n",
    "        # Check if the token is in the likelihood dictionary for the current emotion\n",
    "        if token in prior_probabilities[emotion]:\n",
    "            # Update the posterior probability for the current emotion using the likelihood of the token\n",
    "            posteriors_probabilities[emotion] *= prior_probabilities[emotion][token]\n",
    "\n",
    "# Normalize the posterior probabilities by dividing by the sum of all probabilities\n",
    "sum_posteriors = sum(posteriors_probabilities.values())\n",
    "posteriors_normalized = {emotion: posteriors_probabilities[emotion] / sum_posteriors for emotion in emotions}\n",
    "\n",
    "total_sum_prob = 0\n",
    "# Print the normalized posterior probabilities for each emotion\n",
    "print(\"Normalized posterior probabilities for each emotion:\\n\")\n",
    "for emotion, probability in posteriors_normalized.items():\n",
    "    print(f\"P({emotion}|S) = {probability}\")\n",
    "    total_sum_prob+=probability\n",
    "\n",
    "print(f\"\\nTotal Sum Probabilities = {round(total_sum_prob, 1000)}\")\n",
    "\n",
    "# Determine the predicted emotion as the one with the highest posterior probability\n",
    "predicted_emotion = max(posteriors_normalized, key=posteriors_normalized.get)\n",
    "\n",
    "print(f\"\\nText: {s_text}\")\n",
    "# Print the predicted emotion and the formula used to calculate it\n",
    "print(f\"\\nPredicted emotion: {predicted_emotion}\")"
   ]
  }
 ]
}
